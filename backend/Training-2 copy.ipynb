{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images combined successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Source directories\n",
    "train_dir = '/app/GroceryStoreDataset/dataset/train-1'\n",
    "validation_dir = '/app/GroceryStoreDataset/dataset/test-1'  # The separate validation directory\n",
    "\n",
    "# Destination directory\n",
    "combined_dir = '/app/GroceryStoreDataset/dataset/combined'\n",
    "\n",
    "# Ensure the combined directory exists\n",
    "if not os.path.exists(combined_dir):\n",
    "    os.makedirs(combined_dir)\n",
    "\n",
    "# Function to copy images from source to destination\n",
    "def copy_images(src_dir, dest_dir):\n",
    "    for class_name in os.listdir(src_dir):\n",
    "        class_src_path = os.path.join(src_dir, class_name)\n",
    "        class_dest_path = os.path.join(dest_dir, class_name)\n",
    "\n",
    "        # Create class directory in destination if it doesn't exist\n",
    "        if not os.path.exists(class_dest_path):\n",
    "            os.makedirs(class_dest_path)\n",
    "\n",
    "        # Copy images\n",
    "        for image_name in os.listdir(class_src_path):\n",
    "            src_image_path = os.path.join(class_src_path, image_name)\n",
    "            dest_image_path = os.path.join(class_dest_path, image_name)\n",
    "            shutil.copyfile(src_image_path, dest_image_path)\n",
    "\n",
    "# Copy images from training directory\n",
    "copy_images(train_dir, combined_dir)\n",
    "\n",
    "# Copy images from validation directory\n",
    "copy_images(validation_dir, combined_dir)\n",
    "\n",
    "print(\"Images combined successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2142 images belonging to 81 classes.\n",
      "Found 498 images belonging to 81 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_dir = '/app/GroceryStoreDataset/dataset'\n",
    "data_dir = os.path.join(base_dir, 'combined')\n",
    "\n",
    "datagen =  tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    seed=42\n",
    ")\n",
    "validation_generator =datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2142 images belonging to 81 classes.\n",
      "Found 498 images belonging to 81 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base_dir = '/app/GroceryStoreDataset/dataset'\n",
    "data_dir = os.path.join(base_dir, 'train-1')\n",
    "\n",
    "datagen =  tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "validation_generator =datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Load the VGG16 model pre-trained on ImageNet, without the top layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# Add a logistic layer for the number of classes\n",
    "num_classes = 81 # Assuming each sub-directory in train_dir is a class\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# Freeze the layers of the VGG16 base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='/app/vg16.keras',\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.7404 - loss: 1.0210\n",
      "Epoch 1: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 7s/step - accuracy: 0.7402 - loss: 1.0212 - val_accuracy: 0.7000 - val_loss: 1.0810\n",
      "Epoch 2/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:08\u001b[0m 8s/step - accuracy: 0.7188 - loss: 0.9067"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - accuracy: 0.7188 - loss: 0.9067 - val_accuracy: 0.5000 - val_loss: 1.4648\n",
      "Epoch 3/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.7505 - loss: 0.9808\n",
      "Epoch 3: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 8s/step - accuracy: 0.7504 - loss: 0.9807 - val_accuracy: 0.7167 - val_loss: 1.0441\n",
      "Epoch 4/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:40\u001b[0m 6s/step - accuracy: 0.6562 - loss: 1.1227\n",
      "Epoch 4: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step - accuracy: 0.6562 - loss: 1.1227 - val_accuracy: 0.7778 - val_loss: 0.9796\n",
      "Epoch 5/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.7730 - loss: 0.8895\n",
      "Epoch 5: val_accuracy did not improve from 0.88889\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m584s\u001b[0m 9s/step - accuracy: 0.7727 - loss: 0.8894 - val_accuracy: 0.7396 - val_loss: 0.9607\n",
      "Epoch 6/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:34\u001b[0m 6s/step - accuracy: 0.8438 - loss: 0.7375\n",
      "Epoch 6: val_accuracy improved from 0.88889 to 0.94444, saving model to /app/vg16.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 78ms/step - accuracy: 0.8438 - loss: 0.7375 - val_accuracy: 0.9444 - val_loss: 0.6078\n",
      "Epoch 7/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.7795 - loss: 0.8373\n",
      "Epoch 7: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 8s/step - accuracy: 0.7795 - loss: 0.8371 - val_accuracy: 0.7292 - val_loss: 0.9749\n",
      "Epoch 8/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:25\u001b[0m 6s/step - accuracy: 0.7188 - loss: 0.9560\n",
      "Epoch 8: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 49ms/step - accuracy: 0.7188 - loss: 0.9560 - val_accuracy: 0.6667 - val_loss: 0.9489\n",
      "Epoch 9/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.8096 - loss: 0.7349\n",
      "Epoch 9: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 9s/step - accuracy: 0.8093 - loss: 0.7355 - val_accuracy: 0.7729 - val_loss: 0.8425\n",
      "Epoch 10/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:51\u001b[0m 6s/step - accuracy: 0.8438 - loss: 0.4994\n",
      "Epoch 10: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step - accuracy: 0.8438 - loss: 0.4994 - val_accuracy: 0.7778 - val_loss: 0.8289\n",
      "Epoch 11/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.8118 - loss: 0.7104\n",
      "Epoch 11: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m553s\u001b[0m 8s/step - accuracy: 0.8119 - loss: 0.7102 - val_accuracy: 0.7646 - val_loss: 0.8334\n",
      "Epoch 12/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:30\u001b[0m 6s/step - accuracy: 0.8125 - loss: 0.7052\n",
      "Epoch 12: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - accuracy: 0.8125 - loss: 0.7052 - val_accuracy: 0.7778 - val_loss: 0.7641\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 00:33:54.428707: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67s/step - accuracy: 0.8393 - loss: 0.6221  \n",
      "Epoch 13: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4497s\u001b[0m 69s/step - accuracy: 0.8390 - loss: 0.6225 - val_accuracy: 0.7542 - val_loss: 0.8404\n",
      "Epoch 14/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:32\u001b[0m 6s/step - accuracy: 0.8438 - loss: 0.6649\n",
      "Epoch 14: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 53ms/step - accuracy: 0.8438 - loss: 0.6649 - val_accuracy: 0.7778 - val_loss: 0.8357\n",
      "Epoch 15/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.8129 - loss: 0.6823\n",
      "Epoch 15: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 7s/step - accuracy: 0.8131 - loss: 0.6817 - val_accuracy: 0.7896 - val_loss: 0.8197\n",
      "Epoch 16/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:34\u001b[0m 5s/step - accuracy: 0.8438 - loss: 0.5118\n",
      "Epoch 16: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 44ms/step - accuracy: 0.8438 - loss: 0.5118 - val_accuracy: 0.5556 - val_loss: 0.9858\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.7810 - loss: 0.8229\n",
      "Epoch 1: val_accuracy did not improve from 0.83333\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 8s/step - accuracy: 0.7813 - loss: 0.8219 - val_accuracy: 0.7729 - val_loss: 0.7931\n",
      "Epoch 2/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:54\u001b[0m 8s/step - accuracy: 0.6250 - loss: 0.9863"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.83333\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 68ms/step - accuracy: 0.6250 - loss: 0.9863 - val_accuracy: 0.6667 - val_loss: 0.9851\n",
      "Epoch 3/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.8334 - loss: 0.5975\n",
      "Epoch 3: val_accuracy did not improve from 0.83333\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 8s/step - accuracy: 0.8336 - loss: 0.5969 - val_accuracy: 0.7958 - val_loss: 0.6876\n",
      "Epoch 4/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:34\u001b[0m 6s/step - accuracy: 0.9062 - loss: 0.3933\n",
      "Epoch 4: val_accuracy did not improve from 0.83333\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 45ms/step - accuracy: 0.9062 - loss: 0.3933 - val_accuracy: 0.8333 - val_loss: 0.5891\n",
      "Epoch 5/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.8688 - loss: 0.4444\n",
      "Epoch 5: val_accuracy did not improve from 0.83333\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 8s/step - accuracy: 0.8687 - loss: 0.4446 - val_accuracy: 0.8188 - val_loss: 0.5792\n",
      "Epoch 6/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:39\u001b[0m 6s/step - accuracy: 0.8750 - loss: 0.4714\n",
      "Epoch 6: val_accuracy improved from 0.83333 to 1.00000, saving model to vg16-06-0.36.keras\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 140ms/step - accuracy: 0.8750 - loss: 0.4714 - val_accuracy: 1.0000 - val_loss: 0.3576\n",
      "Epoch 7/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8671 - loss: 0.4626\n",
      "Epoch 7: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 9s/step - accuracy: 0.8673 - loss: 0.4621 - val_accuracy: 0.8104 - val_loss: 0.5864\n",
      "Epoch 8/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:04\u001b[0m 7s/step - accuracy: 0.9062 - loss: 0.2731\n",
      "Epoch 8: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - accuracy: 0.9062 - loss: 0.2731 - val_accuracy: 0.8889 - val_loss: 0.2509\n",
      "Epoch 9/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.8896 - loss: 0.3752\n",
      "Epoch 9: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m658s\u001b[0m 10s/step - accuracy: 0.8897 - loss: 0.3751 - val_accuracy: 0.8583 - val_loss: 0.4740\n",
      "Epoch 10/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:22\u001b[0m 7s/step - accuracy: 0.9062 - loss: 0.4175\n",
      "Epoch 10: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - accuracy: 0.9062 - loss: 0.4175 - val_accuracy: 0.7778 - val_loss: 0.7243\n",
      "Epoch 11/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.8988 - loss: 0.3578\n",
      "Epoch 11: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 8s/step - accuracy: 0.8989 - loss: 0.3574 - val_accuracy: 0.8333 - val_loss: 0.5118\n",
      "Epoch 12/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:43\u001b[0m 7s/step - accuracy: 0.9375 - loss: 0.3125\n",
      "Epoch 12: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9375 - loss: 0.3125 - val_accuracy: 0.8889 - val_loss: 0.3720\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 19:50:33.760883: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.9052 - loss: 0.3097\n",
      "Epoch 13: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 8s/step - accuracy: 0.9052 - loss: 0.3098 - val_accuracy: 0.8521 - val_loss: 0.4597\n",
      "Epoch 14/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:43\u001b[0m 7s/step - accuracy: 0.9375 - loss: 0.2837\n",
      "Epoch 14: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 52ms/step - accuracy: 0.9375 - loss: 0.2837 - val_accuracy: 1.0000 - val_loss: 0.3554\n",
      "Epoch 15/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.9113 - loss: 0.2963\n",
      "Epoch 15: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m596s\u001b[0m 9s/step - accuracy: 0.9114 - loss: 0.2962 - val_accuracy: 0.8792 - val_loss: 0.3824\n",
      "Epoch 16/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:57\u001b[0m 7s/step - accuracy: 0.9688 - loss: 0.2135\n",
      "Epoch 16: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.9688 - loss: 0.2135 - val_accuracy: 0.8889 - val_loss: 0.5191\n",
      "Epoch 17/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 0.9312 - loss: 0.2593\n",
      "Epoch 17: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 8s/step - accuracy: 0.9312 - loss: 0.2591 - val_accuracy: 0.8667 - val_loss: 0.3953\n",
      "Epoch 18/20\n",
      "\u001b[1m 1/66\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:11\u001b[0m 8s/step - accuracy: 0.8750 - loss: 0.4068\n",
      "Epoch 18: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.8750 - loss: 0.4068 - val_accuracy: 0.7222 - val_loss: 0.6029\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    }
   ],
   "source": [
    "'''for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training the model\n",
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "model.save('/app/vg16.keras')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2485 images belonging to 81 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 6s/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "best_model = load_model('/app/vg16.keras')\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "base_dir = '/app/GroceryStoreDataset/dataset'\n",
    "data_dir = os.path.join(base_dir, 'test-1')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Important to not shuffle the data for evaluation\n",
    ")\n",
    "# Get the ground truth labels\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "# Get the class indices\n",
    "class_indices = test_generator.class_indices\n",
    "\n",
    "# Get the list of class names\n",
    "class_names = list(class_indices.keys())\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = model.predict(test_generator, steps=len(test_generator))\n",
    "predicted_labels = predictions.argmax(axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5175050301810865\n",
      "F1 Score: 0.5006692805579736\n",
      "Classification Report:\n",
      "                                        precision    recall  f1-score   support\n",
      "\n",
      "              Alpro-Blueberry-Soyghurt       0.55      1.00      0.71        28\n",
      "                  Alpro-Fresh-Soy-Milk       0.83      0.18      0.29        28\n",
      "                  Alpro-Shelf-Soy-Milk       0.72      0.93      0.81        30\n",
      "                Alpro-Vanilla-Soyghurt       0.00      0.00      0.00        19\n",
      "                                 Anjou       0.27      0.20      0.23        35\n",
      "       Arla-Ecological-Medium-Fat-Milk       0.73      0.66      0.69        29\n",
      "            Arla-Ecological-Sour-Cream       0.71      0.52      0.60        23\n",
      "          Arla-Lactose-Medium-Fat-Milk       0.36      0.88      0.51        25\n",
      "                  Arla-Medium-Fat-Milk       0.71      0.15      0.24        34\n",
      "             Arla-Mild-Vanilla-Yoghurt       0.52      0.59      0.55        27\n",
      "     Arla-Natural-Mild-Low-Fat-Yoghurt       0.62      0.35      0.44        23\n",
      "                  Arla-Natural-Yoghurt       0.94      0.83      0.88        41\n",
      "                       Arla-Sour-Cream       0.25      0.28      0.26        18\n",
      "                        Arla-Sour-Milk       0.82      0.74      0.78        19\n",
      "                    Arla-Standard-Milk       0.69      0.30      0.42        30\n",
      "                             Asparagus       0.13      0.14      0.14        14\n",
      "                             Aubergine       0.60      0.55      0.57        22\n",
      "                               Avocado       0.57      0.70      0.63        40\n",
      "                                Banana       0.77      0.93      0.85        44\n",
      "                           Beef-Tomato       0.50      0.30      0.38        10\n",
      "                     Bravo-Apple-Juice       0.47      0.96      0.63        23\n",
      "                    Bravo-Orange-Juice       0.60      0.39      0.47        31\n",
      "                    Brown-Cap-Mushroom       0.95      0.97      0.96        39\n",
      "                               Cabbage       1.00      0.11      0.19        19\n",
      "                            Cantaloupe       0.59      0.69      0.64        39\n",
      "                               Carrots       0.81      0.81      0.81        42\n",
      "                            Conference       0.82      0.52      0.64        44\n",
      "                              Cucumber       0.48      0.44      0.46        27\n",
      "                         Floury-Potato       0.12      0.12      0.12        16\n",
      "                           Galia-Melon       0.37      0.53      0.44        32\n",
      "     Garant-Ecological-Medium-Fat-Milk       0.78      1.00      0.88        35\n",
      "       Garant-Ecological-Standard-Milk       0.27      0.55      0.36        11\n",
      "                                Garlic       0.79      0.44      0.56        25\n",
      "                                Ginger       0.17      0.07      0.10        15\n",
      "                God-Morgon-Apple-Juice       0.83      0.26      0.40        19\n",
      "               God-Morgon-Orange-Juice       0.82      0.41      0.55        22\n",
      "God-Morgon-Orange-Red-Grapefruit-Juice       0.20      0.26      0.23        19\n",
      "       God-Morgon-Red-Grapefruit-Juice       0.04      0.07      0.05        14\n",
      "                      Golden-Delicious       0.34      0.24      0.29        45\n",
      "                          Granny-Smith       0.37      0.31      0.34        58\n",
      "                     Green-Bell-Pepper       0.59      0.80      0.68        25\n",
      "                        Honeydew-Melon       0.37      0.39      0.38        36\n",
      "                                Kaiser       0.25      0.14      0.18        29\n",
      "                                  Kiwi       0.50      0.31      0.38        45\n",
      "                                  Leek       0.94      0.81      0.87        21\n",
      "                                 Lemon       0.18      0.20      0.19        41\n",
      "                                  Lime       0.00      0.00      0.00        30\n",
      "                                 Mango       0.26      0.23      0.24        31\n",
      "                             Nectarine       0.29      0.26      0.27        35\n",
      "                Oatly-Natural-Oatghurt       0.39      0.97      0.56        30\n",
      "                        Oatly-Oat-Milk       0.58      0.23      0.33        31\n",
      "                                Orange       0.22      0.30      0.25        56\n",
      "                    Orange-Bell-Pepper       0.53      0.69      0.60        26\n",
      "                                Papaya       0.40      0.29      0.33        21\n",
      "                         Passion-Fruit       0.21      0.30      0.25        27\n",
      "                                 Peach       0.44      0.67      0.53        36\n",
      "                             Pineapple       0.77      0.96      0.86        25\n",
      "                             Pink-Lady       0.36      0.86      0.51        59\n",
      "                                  Plum       1.00      0.27      0.43        22\n",
      "                           Pomegranate       0.29      0.08      0.12        25\n",
      "                              Red-Beet       0.64      0.82      0.72        17\n",
      "                       Red-Bell-Pepper       0.53      0.76      0.62        33\n",
      "                         Red-Delicious       0.70      0.98      0.82        50\n",
      "                        Red-Grapefruit       0.16      0.15      0.15        34\n",
      "                        Regular-Tomato       0.85      0.49      0.62        47\n",
      "                            Royal-Gala       0.25      0.20      0.22        64\n",
      "                              Satsumas       0.43      0.41      0.42        68\n",
      "                          Solid-Potato       0.74      0.52      0.61        27\n",
      "                          Sweet-Potato       0.92      0.44      0.60        27\n",
      "                 Tropicana-Apple-Juice       0.79      0.79      0.79        28\n",
      "           Tropicana-Golden-Grapefruit       0.58      0.74      0.65        19\n",
      "                Tropicana-Juice-Smooth       0.81      0.71      0.76        24\n",
      "            Tropicana-Mandarin-Morning       0.44      0.20      0.28        20\n",
      "                 Valio-Vanilla-Yoghurt       0.74      0.84      0.79        31\n",
      "                           Vine-Tomato       0.65      0.79      0.72        43\n",
      "                            Watermelon       0.83      0.98      0.90        46\n",
      "                    Yellow-Bell-Pepper       0.92      0.42      0.58        26\n",
      "                          Yellow-Onion       0.66      0.57      0.61        37\n",
      "              Yoggi-Strawberry-Yoghurt       0.51      0.62      0.56        32\n",
      "                 Yoggi-Vanilla-Yoghurt       0.00      0.00      0.00        18\n",
      "                              Zucchini       0.88      0.52      0.65        29\n",
      "\n",
      "                              accuracy                           0.52      2485\n",
      "                             macro avg       0.54      0.49      0.48      2485\n",
      "                          weighted avg       0.55      0.52      0.50      2485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate F1 score (macro, micro, or weighted)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
